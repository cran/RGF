<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Lampros Mouselimis" />

<meta name="date" content="2018-07-22" />

<title>Regularized Greedy Forest in R</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Regularized Greedy Forest in R</h1>
<h4 class="author"><em>Lampros Mouselimis</em></h4>
<h4 class="date"><em>2018-07-22</em></h4>



<p>The RGF package is a wrapper of the <a href="https://github.com/fukatani/rgf_python"><em>Regularized Greedy Forest</em></a> <em>python</em> package, which also includes a <a href="https://github.com/baidu/fast_rgf"><em>Multi-core implementation (FastRGF)</em></a>. Portability from Python to R was made possible using the <a href="https://github.com/rstudio/reticulate">reticulate</a> package and the installation requires basic knowledge of Python. Except for the Linux Operating System, the installation on Macintosh and Windows might be somehow cumbersome (on windows the package currently can be used only from within the command prompt). Detailed installation instructions for all three Operating Systems can be found in the <em>README.md</em> file and in the <a href="https://github.com/fukatani/rgf_python"><em>rgf_python</em></a> Github repository.</p>
<p><br></p>
<p>The <em>Regularized Greedy Forest</em> algorithm is explained in detail in the paper <a href="https://arxiv.org/abs/1109.0887"><em>Rie Johnson and Tong Zhang, Learning Nonlinear Functions Using Regularized Greedy Forest</em></a>. A small synopsis would be <em>“… the resulting method, which we refer to as regularized greedy forest (RGF), integrates two ideas: one is to include tree-structured regularization into the learning formulation; and the other is to employ the fully-corrective regularized greedy algorithm ….”</em>.</p>
<p><br></p>
<p>At the time of writing this Vignette (11 - 02 - 2018), there isn’t a corresponding implementation of the algorithm in the R language, so I decided to port the Python package in R taking advantage of the reticulate package. In the next lines, I will explain the functionality of the package and I compare RGF with other similar implementations, such as <a href="https://github.com/imbs-hl/ranger"><em>ranger</em></a> (random forest algorithm) and <a href="https://github.com/dmlc/xgboost/tree/master/R-package"><em>xgboost</em></a> (gradient boosting algorithm), in terms of time efficiency and error rate improvement.</p>
<p><br></p>
<div id="the-rgf-package" class="section level4">
<h4>The RGF package</h4>
<p><br></p>
<p>The <em>RGF</em> package includes the following R6-classes / functions,</p>
<p><br></p>
<div id="classes" class="section level5">
<h5><strong>classes</strong></h5>
<p><br></p>
<table>
<thead>
<tr class="header">
<th align="center">RGF_Regressor</th>
<th align="center">RGF_Classifier</th>
<th align="center">FastRGF_Regressor</th>
<th align="center">FastRGF_Classifier</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">fit()</td>
<td align="center">fit(()</td>
<td align="center">fit()</td>
<td align="center">fit()</td>
</tr>
<tr class="even">
<td align="center">predict()</td>
<td align="center">predict()</td>
<td align="center">predict()</td>
<td align="center">predict()</td>
</tr>
<tr class="odd">
<td align="center">cleanup()</td>
<td align="center">predict_proba()</td>
<td align="center">cleanup()</td>
<td align="center">predict_proba()</td>
</tr>
<tr class="even">
<td align="center">get_params()</td>
<td align="center">cleanup()</td>
<td align="center">get_params()</td>
<td align="center">cleanup()</td>
</tr>
<tr class="odd">
<td align="center">score()</td>
<td align="center">get_params()</td>
<td align="center">score()</td>
<td align="center">get_params()</td>
</tr>
<tr class="even">
<td align="center">feature_importances()</td>
<td align="center">score()</td>
<td align="center"></td>
<td align="center">score()</td>
</tr>
<tr class="odd">
<td align="center">dump_model()</td>
<td align="center">feature_importances()</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center">dump_model()</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p><br></p>
</div>
<div id="functions" class="section level5">
<h5><strong>functions</strong></h5>
<p><strong>UPDATE 10-05-2018</strong> : Beginning from version <strong>1.0.3</strong> the <strong>dgCMatrix_2scipy_sparse</strong> function was renamed to <strong>TO_scipy_sparse</strong> and now accepts either a <em>dgCMatrix</em> or a <em>dgRMatrix</em> as input. The appropriate format for the RGF package in case of sparse matrices is the <strong>dgCMatrix</strong> format (<em>scipy.sparse.csc_matrix</em>)</p>
<p><br></p>
<table>
<thead>
<tr class="header">
<th align="left">TO_scipy_sparse()</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="left">RGF_cleanup_temp_files()</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="left">mat_2scipy_sparse()</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<p><br></p>
<p>The package documentation includes details and examples for all R6-classes and functions. In the following code chunks, I’ll explain how a user can work with sparse matrices as all RGF algorithms (besides a dense matrix) <strong>require a python sparse matrix as input</strong>.</p>
<p><br></p>
</div>
</div>
<div id="sparse-matrices-as-input" class="section level4">
<h4>Sparse matrices as input</h4>
<p><br></p>
<p>The RGF package includes two functions (<strong>mat_2scipy_sparse</strong> and <strong>TO_scipy_sparse</strong>) which allow the user to convert from a <em>matrix</em> / <em>sparse matrix</em> (<em>dgCMatrix</em>, <em>dgRMatrix</em>) to a <em>scipy sparse matrix</em> (<em>scipy.sparse.csc_matrix</em>, <em>scipy.sparse.csr_matrix</em>),</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(RGF)

<span class="co"># conversion from a matrix object to a scipy sparse matrix</span>
<span class="co">#----------------------------------------------------------</span>

<span class="kw">set.seed</span>(<span class="dv">1</span>)

x =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dv">1000</span>), <span class="dt">nrow =</span> <span class="dv">100</span>, <span class="dt">ncol =</span> <span class="dv">10</span>)

x_sparse =<span class="st"> </span><span class="kw">mat_2scipy_sparse</span>(x, <span class="dt">format =</span> <span class="st">&quot;sparse_row_matrix&quot;</span>)

<span class="kw">print</span>(<span class="kw">dim</span>(x))

[<span class="dv">1</span>] <span class="dv">100</span>  <span class="dv">10</span>

<span class="kw">print</span>(x_sparse<span class="op">$</span>shape)

(<span class="dv">100</span>, <span class="dv">10</span>)</code></pre></div>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># conversion from a dgCMatrix object to a scipy sparse matrix</span>
<span class="co">#-------------------------------------------------------------</span>

data =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>)


<span class="co"># 'dgCMatrix' sparse matrix</span>
<span class="co">#--------------------------</span>

dgcM =<span class="st"> </span>Matrix<span class="op">::</span><span class="kw">Matrix</span>(<span class="dt">data =</span> data, <span class="dt">nrow =</span> <span class="dv">3</span>,

                      <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>,

                      <span class="dt">sparse =</span> <span class="ot">TRUE</span>)

<span class="kw">print</span>(<span class="kw">dim</span>(dgcM))

[<span class="dv">1</span>] <span class="dv">3</span> <span class="dv">3</span>

x_sparse =<span class="st"> </span><span class="kw">TO_scipy_sparse</span>(dgcM)

<span class="kw">print</span>(x_sparse<span class="op">$</span>shape)

(<span class="dv">3</span>, <span class="dv">3</span>)


<span class="co"># 'dgRMatrix' sparse matrix</span>
<span class="co">#--------------------------</span>

dgrM =<span class="st"> </span><span class="kw">as</span>(dgcM, <span class="st">&quot;RsparseMatrix&quot;</span>)

<span class="kw">class</span>(dgrM)

<span class="co"># [1] &quot;dgRMatrix&quot;</span>
<span class="co"># attr(,&quot;package&quot;)</span>
<span class="co"># [1] &quot;Matrix&quot;</span>

<span class="kw">print</span>(<span class="kw">dim</span>(dgrM))

[<span class="dv">1</span>] <span class="dv">3</span> <span class="dv">3</span>

res_dgr =<span class="st"> </span><span class="kw">TO_scipy_sparse</span>(dgrM)

<span class="kw">print</span>(res_dgr<span class="op">$</span>shape)

(<span class="dv">3</span>, <span class="dv">3</span>)</code></pre></div>
<p><br></p>
</div>
<div id="comparison-of-rgf-with-ranger-and-xgboost" class="section level4">
<h4>Comparison of RGF with ranger and xgboost</h4>
<p><br></p>
<p>First the data, libraries and cross-validation function will be inputted (the <em>MLmetrics</em> library is also required),</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(Boston, <span class="dt">package =</span> <span class="st">'KernelKnn'</span>)

<span class="kw">library</span>(RGF)
<span class="kw">library</span>(ranger)
<span class="kw">library</span>(xgboost)



<span class="co"># shuffling function for cross-validation folds</span>
<span class="co">#-----------------------------------------------</span>


func_shuffle =<span class="st"> </span><span class="cf">function</span>(vec, <span class="dt">times =</span> <span class="dv">10</span>) {

  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>times) {

    out =<span class="st"> </span><span class="kw">sample</span>(vec, <span class="kw">length</span>(vec))
  }
  out
}


<span class="co"># cross-validation folds [ regression]</span>
<span class="co">#-------------------------------------</span>


regr_folds =<span class="st"> </span><span class="cf">function</span>(folds, RESP, <span class="dt">stratified =</span> <span class="ot">FALSE</span>) {

  <span class="cf">if</span> (<span class="kw">is.factor</span>(RESP)) {

    <span class="kw">stop</span>(<span class="kw">simpleError</span>(<span class="st">&quot;this function is meant for regression for classification </span>
<span class="st">                     use the 'class_folds' function&quot;</span>))
  }

  samp_vec =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span>folds, folds)

  sort_names =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">'fold_'</span>, <span class="dv">1</span><span class="op">:</span>folds)


  <span class="cf">if</span> (stratified <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {

    stratif =<span class="st"> </span><span class="kw">cut</span>(RESP, <span class="dt">breaks =</span> folds)

    clas =<span class="st"> </span><span class="kw">lapply</span>(<span class="kw">unique</span>(stratif), <span class="cf">function</span>(x) <span class="kw">which</span>(stratif <span class="op">==</span><span class="st"> </span>x))

    len =<span class="st"> </span><span class="kw">lapply</span>(clas, <span class="cf">function</span>(x) <span class="kw">length</span>(x))

    prop =<span class="st"> </span><span class="kw">lapply</span>(len, <span class="cf">function</span>(y) <span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(samp_vec), <span class="cf">function</span>(x) 
      <span class="kw">round</span>(y <span class="op">*</span><span class="st"> </span>samp_vec[x])))

    repl =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(prop, <span class="cf">function</span>(x) <span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x), <span class="cf">function</span>(y) 
      <span class="kw">rep</span>(<span class="kw">paste0</span>(<span class="st">'fold_'</span>, y), x[y]))))

    spl =<span class="st"> </span><span class="kw">suppressWarnings</span>(<span class="kw">split</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(RESP), repl))}

  <span class="cf">else</span> {

    prop =<span class="st"> </span><span class="kw">lapply</span>(<span class="kw">length</span>(RESP), <span class="cf">function</span>(y) <span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(samp_vec), 
                                                   <span class="cf">function</span>(x) <span class="kw">round</span>(y <span class="op">*</span><span class="st"> </span>samp_vec[x])))

    repl =<span class="st"> </span><span class="kw">func_shuffle</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(prop, <span class="cf">function</span>(x) 
      <span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x), <span class="cf">function</span>(y) <span class="kw">rep</span>(<span class="kw">paste0</span>(<span class="st">'fold_'</span>, y), x[y])))))

    spl =<span class="st"> </span><span class="kw">suppressWarnings</span>(<span class="kw">split</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(RESP), repl))
  }

  spl =<span class="st"> </span>spl[sort_names]

  <span class="cf">if</span> (<span class="kw">length</span>(<span class="kw">table</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(spl, <span class="cf">function</span>(x) <span class="kw">length</span>(x))))) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {

    <span class="kw">warning</span>(<span class="st">'the folds are not equally split'</span>)
  }

  <span class="cf">if</span> (<span class="kw">length</span>(<span class="kw">unlist</span>(spl)) <span class="op">!=</span><span class="st"> </span><span class="kw">length</span>(RESP)) {

    <span class="kw">stop</span>(<span class="kw">simpleError</span>(<span class="st">&quot;the length of the splits are not equal with the length </span>
<span class="st">                     of the response&quot;</span>))
  }

  spl
}</code></pre></div>
<p><br></p>
<table>
<thead>
<tr class="header">
<th>single threaded [ small data set ]</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<p><br></p>
<p>In the next code chunk, I’ll perform 5-fold cross-validation using the Boston dataset and I’ll compare time execution and error rate for all three algorithms (<strong>without doing hyper-parameter tuning</strong>),</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">NUM_FOLDS =<span class="st"> </span><span class="dv">5</span>

<span class="kw">set.seed</span>(<span class="dv">1</span>)
FOLDS =<span class="st"> </span><span class="kw">regr_folds</span>(<span class="dt">folds =</span> NUM_FOLDS, Boston[, <span class="st">'medv'</span>], <span class="dt">stratified =</span> T)


boston_rgf_te =<span class="st"> </span>boston_ranger_te =<span class="st"> </span>boston_xgb_te =<span class="st"> </span>boston_rgf_time =<span class="st"> </span>
<span class="st">  </span>boston_ranger_time =<span class="st"> </span>boston_xgb_time =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, NUM_FOLDS)


<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(FOLDS)) {

  <span class="kw">cat</span>(<span class="st">&quot;fold : &quot;</span>, i, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)

  samp =<span class="st"> </span><span class="kw">unlist</span>(FOLDS[<span class="op">-</span>i])
  samp_ =<span class="st"> </span><span class="kw">unlist</span>(FOLDS[i])


  <span class="co"># RGF</span>
  <span class="co">#----</span>

  rgf_start =<span class="st"> </span><span class="kw">Sys.time</span>()

  init_regr =<span class="st"> </span>RGF_Regressor<span class="op">$</span><span class="kw">new</span>(<span class="dt">l2 =</span> <span class="fl">0.1</span>)

  init_regr<span class="op">$</span><span class="kw">fit</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(Boston[samp, <span class="op">-</span><span class="kw">ncol</span>(Boston)]), <span class="dt">y =</span> Boston[samp, <span class="kw">ncol</span>(Boston)])

  pr_te =<span class="st"> </span>init_regr<span class="op">$</span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(Boston[samp_, <span class="op">-</span><span class="kw">ncol</span>(Boston)]))

  rgf_end =<span class="st"> </span><span class="kw">Sys.time</span>()

  boston_rgf_time[i] =<span class="st"> </span>rgf_end <span class="op">-</span><span class="st"> </span>rgf_start

  boston_rgf_te[i] =<span class="st"> </span>MLmetrics<span class="op">::</span><span class="kw">RMSE</span>(Boston[samp_, <span class="st">'medv'</span>], pr_te)


  <span class="co"># ranger</span>
  <span class="co">#-------</span>

  ranger_start =<span class="st"> </span><span class="kw">Sys.time</span>()

  fit =<span class="st"> </span><span class="kw">ranger</span>(<span class="dt">dependent.variable.name =</span> <span class="st">&quot;medv&quot;</span>, <span class="dt">data =</span> Boston[samp, ], <span class="dt">write.forest =</span> <span class="ot">TRUE</span>, 
               
               <span class="dt">probability =</span> F, <span class="dt">num.threads =</span> <span class="dv">1</span>, <span class="dt">num.trees =</span> <span class="dv">500</span>, <span class="dt">verbose =</span> T, 
               
               <span class="dt">classification =</span> F, <span class="dt">mtry =</span> <span class="ot">NULL</span>, <span class="dt">min.node.size =</span> <span class="dv">5</span>, <span class="dt">keep.inbag =</span> T)

  pred_te =<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">data =</span> Boston[samp_, <span class="op">-</span><span class="kw">ncol</span>(Boston)], <span class="dt">type =</span> <span class="st">'se'</span>)<span class="op">$</span>predictions

  ranger_end =<span class="st"> </span><span class="kw">Sys.time</span>()

  boston_ranger_time[i] =<span class="st"> </span>ranger_end <span class="op">-</span><span class="st"> </span>ranger_start

  boston_ranger_te[i] =<span class="st"> </span>MLmetrics<span class="op">::</span><span class="kw">RMSE</span>(Boston[samp_, <span class="st">'medv'</span>], pred_te)


  <span class="co"># xgboost</span>
  <span class="co">#--------</span>

  xgb_start =<span class="st"> </span><span class="kw">Sys.time</span>()

  dtrain &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(<span class="dt">data =</span> <span class="kw">as.matrix</span>(Boston[samp, <span class="op">-</span><span class="kw">ncol</span>(Boston)]), 
                        
                        <span class="dt">label =</span> Boston[samp, <span class="kw">ncol</span>(Boston)])

  dtest &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(<span class="dt">data =</span> <span class="kw">as.matrix</span>(Boston[samp_, <span class="op">-</span><span class="kw">ncol</span>(Boston)]), 
                       
                       <span class="dt">label =</span> Boston[samp_, <span class="kw">ncol</span>(Boston)])

  
  watchlist &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">train =</span> dtrain, <span class="dt">test =</span> dtest)

  
  param =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;objective&quot;</span> =<span class="st"> &quot;reg:linear&quot;</span>, <span class="st">&quot;bst:eta&quot;</span> =<span class="st"> </span><span class="fl">0.05</span>, <span class="st">&quot;max_depth&quot;</span> =<span class="st"> </span><span class="dv">4</span>, 
               
               <span class="st">&quot;subsample&quot;</span> =<span class="st"> </span><span class="fl">0.85</span>, <span class="st">&quot;colsample_bytree&quot;</span> =<span class="st"> </span><span class="fl">0.85</span>, <span class="st">&quot;booster&quot;</span> =<span class="st"> &quot;gbtree&quot;</span>,
               
               <span class="st">&quot;nthread&quot;</span> =<span class="st"> </span><span class="dv">1</span>)

  fit =<span class="st"> </span><span class="kw">xgb.train</span>(param, dtrain, <span class="dt">nround =</span> <span class="dv">500</span>, <span class="dt">print_every_n  =</span> <span class="dv">100</span>, <span class="dt">watchlist =</span> watchlist,

                  <span class="dt">early_stopping_rounds =</span> <span class="dv">20</span>, <span class="dt">maximize =</span> <span class="ot">FALSE</span>, <span class="dt">verbose =</span> <span class="dv">0</span>)

  p_te =<span class="st"> </span>xgboost<span class="op">:::</span><span class="kw">predict.xgb.Booster</span>(fit, <span class="kw">as.matrix</span>(Boston[samp_, <span class="op">-</span><span class="kw">ncol</span>(Boston)]), 
                                       
                                       <span class="dt">ntreelimit =</span> fit<span class="op">$</span>best_iteration)

  xgb_end =<span class="st"> </span><span class="kw">Sys.time</span>()

  boston_xgb_time[i] =<span class="st"> </span>xgb_end <span class="op">-</span><span class="st"> </span>xgb_start

  boston_xgb_te[i] =<span class="st"> </span>MLmetrics<span class="op">::</span><span class="kw">RMSE</span>(Boston[samp_, <span class="st">'medv'</span>], p_te)
}


fold <span class="op">:</span><span class="st">  </span><span class="dv">1</span> 
fold <span class="op">:</span><span class="st">  </span><span class="dv">2</span> 
fold <span class="op">:</span><span class="st">  </span><span class="dv">3</span> 
fold <span class="op">:</span><span class="st">  </span><span class="dv">4</span> 
fold <span class="op">:</span><span class="st">  </span><span class="dv">5</span> 



<span class="kw">cat</span>(<span class="st">&quot;total time rgf 5 fold cross-validation : &quot;</span>, <span class="kw">sum</span>(boston_rgf_time), 
    <span class="st">&quot; mean rmse on test data : &quot;</span>, <span class="kw">mean</span>(boston_rgf_te), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)

<span class="kw">cat</span>(<span class="st">&quot;total time ranger 5 fold cross-validation : &quot;</span>, <span class="kw">sum</span>(boston_ranger_time), 
    <span class="st">&quot; mean rmse on test data : &quot;</span>, <span class="kw">mean</span>(boston_ranger_te), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)

<span class="kw">cat</span>(<span class="st">&quot;total time xgb 5 fold cross-validation : &quot;</span>, <span class="kw">sum</span>(boston_xgb_time),
    <span class="st">&quot; mean rmse on test data : &quot;</span>, <span class="kw">mean</span>(boston_xgb_te), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre></div>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">total time rgf <span class="dv">5</span> fold cross<span class="op">-</span>validation <span class="op">:</span><span class="st">  </span><span class="fl">0.7730639</span>  mean rmse on test data <span class="op">:</span><span class="st">  </span><span class="fl">3.832135</span> 
total time ranger <span class="dv">5</span> fold cross<span class="op">-</span>validation <span class="op">:</span><span class="st">  </span><span class="fl">3.826846</span>  mean rmse on test data <span class="op">:</span><span class="st">  </span><span class="fl">4.17419</span> 
total time xgb <span class="dv">5</span> fold cross<span class="op">-</span>validation <span class="op">:</span><span class="st">  </span><span class="fl">0.4316094</span>  mean rmse on test data <span class="op">:</span><span class="st">  </span><span class="fl">3.949122</span> </code></pre></div>
<p><br></p>
<table>
<colgroup>
<col width="100%"></col>
</colgroup>
<thead>
<tr class="header">
<th>5 threads [ high dimensional data set and presence of multicollinearity ]</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<p><br></p>
<p>For the high-dimensional data (can be downloaded from my <a href="https://github.com/mlampros/DataSets">Github repository</a>) I’ll use the <em>FastRGF_Regressor</em> rather than the RGF_Regressor (comparison <strong>without doing hyper-parameter tuning</strong>),</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># download the data from my Github repository (tested on a Linux OS)</span>

<span class="kw">system</span>(<span class="st">&quot;wget </span>
<span class="st">       https://raw.githubusercontent.com/mlampros/DataSets/master/africa_soil_train_data.zip&quot;</span>)


<span class="co"># load the data in the R session</span>

train_dat =<span class="st"> </span><span class="kw">read.table</span>(<span class="kw">unz</span>(<span class="st">&quot;africa_soil_train_data.zip&quot;</span>, <span class="st">&quot;train.csv&quot;</span>), <span class="dt">nrows =</span> <span class="dv">1157</span>, 
                       
                       <span class="dt">header =</span> T, <span class="dt">quote =</span> <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>)


<span class="co"># c(&quot;Ca&quot;, &quot;P&quot;, &quot;pH&quot;, &quot;SOC&quot;, &quot;Sand&quot;) : response variables            </span>


<span class="co"># exclude response-variables and factor variable</span>

x =<span class="st"> </span>train_dat[, <span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">which</span>(<span class="kw">colnames</span>(train_dat) <span class="op">%in%</span><span class="st"> </span>
<span class="st">                              </span><span class="kw">c</span>(<span class="st">&quot;Ca&quot;</span>, <span class="st">&quot;P&quot;</span>, <span class="st">&quot;pH&quot;</span>, <span class="st">&quot;SOC&quot;</span>, <span class="st">&quot;Sand&quot;</span>, <span class="st">&quot;Depth&quot;</span>)))]


<span class="co"># take (randomly) the first of the responses for train</span>

y =<span class="st"> </span>train_dat[, <span class="st">&quot;Ca&quot;</span>]


<span class="co"># dataset for ranger</span>

tmp_rg_dat =<span class="st"> </span><span class="kw">cbind</span>(<span class="dt">Ca =</span> y, x)


<span class="co"># cross-validation folds</span>

<span class="kw">set.seed</span>(<span class="dv">2</span>)
FOLDS =<span class="st"> </span><span class="kw">regr_folds</span>(<span class="dt">folds =</span> NUM_FOLDS, y, <span class="dt">stratified =</span> T)


highdim_rgf_te =<span class="st"> </span>highdim_ranger_te =<span class="st"> </span>highdim_xgb_te =<span class="st"> </span>highdim_rgf_time =<span class="st"> </span>
<span class="st">  </span>highdim_ranger_time =<span class="st"> </span>highdim_xgb_time =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, NUM_FOLDS)


<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(FOLDS)) {

  <span class="kw">cat</span>(<span class="st">&quot;fold : &quot;</span>, i, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)

  new_samp =<span class="st"> </span><span class="kw">unlist</span>(FOLDS[<span class="op">-</span>i])
  new_samp_ =<span class="st"> </span><span class="kw">unlist</span>(FOLDS[i])


  <span class="co"># RGF</span>
  <span class="co">#----</span>

  rgf_start =<span class="st"> </span><span class="kw">Sys.time</span>()

  init_regr =<span class="st"> </span>FastRGF_Regressor<span class="op">$</span><span class="kw">new</span>(<span class="dt">n_jobs =</span> <span class="dv">5</span>, <span class="dt">l2 =</span> <span class="fl">0.1</span>)       <span class="co"># added 'l2' regularization</span>

  init_regr<span class="op">$</span><span class="kw">fit</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(x[new_samp, ]), <span class="dt">y =</span> y[new_samp])

  pr_te =<span class="st"> </span>init_regr<span class="op">$</span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(x[new_samp_, ]))

  rgf_end =<span class="st"> </span><span class="kw">Sys.time</span>()

  highdim_rgf_time[i] =<span class="st"> </span>rgf_end <span class="op">-</span><span class="st"> </span>rgf_start

  highdim_rgf_te[i] =<span class="st"> </span>MLmetrics<span class="op">::</span><span class="kw">RMSE</span>(y[new_samp_], pr_te)


  <span class="co"># ranger</span>
  <span class="co">#-------</span>

  ranger_start =<span class="st"> </span><span class="kw">Sys.time</span>()
  

  fit =<span class="st"> </span><span class="kw">ranger</span>(<span class="dt">dependent.variable.name =</span> <span class="st">&quot;Ca&quot;</span>, <span class="dt">data =</span> tmp_rg_dat[new_samp, ], 
               
               <span class="dt">write.forest =</span> <span class="ot">TRUE</span>, <span class="dt">probability =</span> F, <span class="dt">num.threads =</span> <span class="dv">5</span>, <span class="dt">num.trees =</span> <span class="dv">500</span>,
               
               <span class="dt">verbose =</span> T, <span class="dt">classification =</span> F, <span class="dt">mtry =</span> <span class="ot">NULL</span>, <span class="dt">min.node.size =</span> <span class="dv">5</span>, 
               
               <span class="dt">keep.inbag =</span> T)
  

  pred_te =<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">data =</span> x[new_samp_, ], <span class="dt">type =</span> <span class="st">'se'</span>)<span class="op">$</span>predictions

  ranger_end =<span class="st"> </span><span class="kw">Sys.time</span>()

  highdim_ranger_time[i] =<span class="st"> </span>ranger_end <span class="op">-</span><span class="st"> </span>ranger_start

  highdim_ranger_te[i] =<span class="st"> </span>MLmetrics<span class="op">::</span><span class="kw">RMSE</span>(y[new_samp_], pred_te)


  <span class="co"># xgboost</span>
  <span class="co">#--------</span>

  xgb_start =<span class="st"> </span><span class="kw">Sys.time</span>()

  dtrain &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(<span class="dt">data =</span> <span class="kw">as.matrix</span>(x[new_samp, ]), <span class="dt">label =</span> y[new_samp])

  dtest &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(<span class="dt">data =</span> <span class="kw">as.matrix</span>(x[new_samp_, ]), <span class="dt">label =</span> y[new_samp_])

  watchlist &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">train =</span> dtrain, <span class="dt">test =</span> dtest)

  param =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;objective&quot;</span> =<span class="st"> &quot;reg:linear&quot;</span>, <span class="st">&quot;bst:eta&quot;</span> =<span class="st"> </span><span class="fl">0.05</span>, <span class="st">&quot;max_depth&quot;</span> =<span class="st"> </span><span class="dv">6</span>, 
               
               <span class="st">&quot;subsample&quot;</span> =<span class="st"> </span><span class="fl">0.85</span>, <span class="st">&quot;colsample_bytree&quot;</span> =<span class="st"> </span><span class="fl">0.85</span>, <span class="st">&quot;booster&quot;</span> =<span class="st"> &quot;gbtree&quot;</span>,
               
               <span class="st">&quot;nthread&quot;</span> =<span class="st"> </span><span class="dv">5</span>)                        <span class="co"># &quot;lambda&quot; = 0.1 does not improve RMSE</span>

  fit =<span class="st"> </span><span class="kw">xgb.train</span>(param, dtrain, <span class="dt">nround =</span> <span class="dv">500</span>, <span class="dt">print_every_n  =</span> <span class="dv">100</span>, <span class="dt">watchlist =</span> watchlist,

                  <span class="dt">early_stopping_rounds =</span> <span class="dv">20</span>, <span class="dt">maximize =</span> <span class="ot">FALSE</span>, <span class="dt">verbose =</span> <span class="dv">0</span>)

  p_te =<span class="st"> </span>xgboost<span class="op">:::</span><span class="kw">predict.xgb.Booster</span>(fit, <span class="kw">as.matrix</span>(x[new_samp_, ]), 
                                       <span class="dt">ntreelimit =</span> fit<span class="op">$</span>best_iteration)

  xgb_end =<span class="st"> </span><span class="kw">Sys.time</span>()

  highdim_xgb_time[i] =<span class="st"> </span>xgb_end <span class="op">-</span><span class="st"> </span>xgb_start

  highdim_xgb_te[i] =<span class="st"> </span>MLmetrics<span class="op">::</span><span class="kw">RMSE</span>(y[new_samp_], p_te)
}


fold <span class="op">:</span><span class="st">  </span><span class="dv">1</span> 
fold <span class="op">:</span><span class="st">  </span><span class="dv">2</span> 
fold <span class="op">:</span><span class="st">  </span><span class="dv">3</span> 
fold <span class="op">:</span><span class="st">  </span><span class="dv">4</span> 
fold <span class="op">:</span><span class="st">  </span><span class="dv">5</span> 


<span class="kw">cat</span>(<span class="st">&quot;total time rgf 5 fold cross-validation : &quot;</span>, <span class="kw">sum</span>(highdim_rgf_time), 
    <span class="st">&quot; mean rmse on test data : &quot;</span>, <span class="kw">mean</span>(highdim_rgf_te), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)

<span class="kw">cat</span>(<span class="st">&quot;total time ranger 5 fold cross-validation : &quot;</span>, <span class="kw">sum</span>(highdim_ranger_time), 
    <span class="st">&quot; mean rmse on test data : &quot;</span>, <span class="kw">mean</span>(highdim_ranger_te), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)

<span class="kw">cat</span>(<span class="st">&quot;total time xgb 5 fold cross-validation : &quot;</span>, <span class="kw">sum</span>(highdim_xgb_time), 
    <span class="st">&quot; mean rmse on test data : &quot;</span>, <span class="kw">mean</span>(highdim_xgb_te), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre></div>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">total time rgf <span class="dv">5</span> fold cross<span class="op">-</span>validation <span class="op">:</span><span class="st">  </span><span class="fl">92.31971</span>  mean rmse on test data <span class="op">:</span><span class="st">  </span><span class="fl">0.5155166</span>
total time ranger <span class="dv">5</span> fold cross<span class="op">-</span>validation <span class="op">:</span><span class="st">  </span><span class="fl">27.32866</span>  mean rmse on test data <span class="op">:</span><span class="st">  </span><span class="fl">0.5394164</span>
total time xgb <span class="dv">5</span> fold cross<span class="op">-</span>validation <span class="op">:</span><span class="st">  </span><span class="fl">30.48834</span>  mean rmse on test data <span class="op">:</span><span class="st">  </span><span class="fl">0.5453544</span></code></pre></div>
<p><br></p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
